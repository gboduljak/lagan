{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patch_sampler import PatchSampler\n",
    "from paper_patch_sampler import PatchSampleF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "patch_embedding_dim = 256\n",
    "num_patches_per_layer = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_patch_sampler = PatchSampler(\n",
    "    patch_embedding_dim=patch_embedding_dim,\n",
    "    num_patches_per_layer=num_patches_per_layer,\n",
    "    device=torch.device('cpu')\n",
    ")\n",
    "paper_patch_sampler = PatchSampleF(\n",
    "    nc=patch_embedding_dim,\n",
    "    device=torch.device('cpu'),\n",
    "    use_mlp=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "  layer_outs = [\n",
    "      torch.randn((batch_size, 128, 128, 128)),\n",
    "      torch.randn((batch_size, 256, 64, 64)),\n",
    "      torch.randn((batch_size, 512, 32, 32)),\n",
    "  ]\n",
    "  cust_patches, cust_idx = custom_patch_sampler(\n",
    "      layer_outs,\n",
    "      apply_mlp=True\n",
    "  )\n",
    "  for layer_idx in range(len(cust_patches)):\n",
    "    setattr(\n",
    "        paper_patch_sampler,\n",
    "        f'mlp_{layer_idx}',\n",
    "        getattr(\n",
    "            custom_patch_sampler,\n",
    "            f'mlp_{layer_idx}',\n",
    "        )\n",
    "    )\n",
    "  paper_patch_sampler.mlp_init = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting correct patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_patch_sampler = PatchSampler(\n",
    "    patch_embedding_dim=patch_embedding_dim,\n",
    "    num_patches_per_layer=num_patches_per_layer,\n",
    "    device=torch.device('cpu')\n",
    ")\n",
    "paper_patch_sampler = PatchSampleF(\n",
    "    nc=patch_embedding_dim,\n",
    "    device=torch.device('cpu'),\n",
    "    use_mlp=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "iters = 100\n",
    "batch_size = 1\n",
    "\n",
    "for _ in range(iters):\n",
    "  layer_outs = [\n",
    "      torch.randn((batch_size, 128, 128, 128)),\n",
    "      torch.randn((batch_size, 256, 64, 64)),\n",
    "      torch.randn((batch_size, 512, 32, 32)),\n",
    "  ]\n",
    "  with torch.no_grad():\n",
    "    custom_patches, patches_idx = custom_patch_sampler(\n",
    "        layer_outs,\n",
    "        apply_mlp=False\n",
    "    )\n",
    "\n",
    "    for (layer_idx, layer_out) in enumerate(layer_outs):\n",
    "      rearrange_layer_out = rearrange(layer_out, 'b c h w -> b (h w) c')\n",
    "\n",
    "      for batch_idx in range(batch_size):\n",
    "        expected_patches = torch.index_select(input=rearrange_layer_out[0],\n",
    "                                              index=patches_idx[layer_idx][0], dim=0)\n",
    "        expected_normalized_patches = F.normalize(expected_patches, p=2, dim=-1)\n",
    "\n",
    "        assert torch.equal(expected_normalized_patches,\n",
    "                           custom_patches[layer_idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "\n",
    "iters = 100\n",
    "batch_size = 4\n",
    "\n",
    "for _ in range(iters):\n",
    "  layer_outs = [\n",
    "      torch.randn((batch_size, 128, 128, 128)),\n",
    "      torch.randn((batch_size, 256, 64, 64)),\n",
    "      torch.randn((batch_size, 512, 32, 32)),\n",
    "  ]\n",
    "  with torch.no_grad():\n",
    "    custom_patches, patches_idx = custom_patch_sampler(\n",
    "        layer_outs,\n",
    "        apply_mlp=False\n",
    "    )\n",
    "\n",
    "    for (layer_idx, layer_out) in enumerate(layer_outs):\n",
    "      rearrange_layer_out = rearrange(layer_out, 'b c h w -> b (h w) c')\n",
    "\n",
    "      for batch_idx in range(batch_size):\n",
    "        expected_patches = torch.index_select(input=rearrange_layer_out[batch_idx],\n",
    "                                              index=patches_idx[layer_idx][batch_idx], dim=0)\n",
    "        expected_normalized_patches = F.normalize(expected_patches, p=2, dim=-1)\n",
    "\n",
    "        assert torch.equal(expected_normalized_patches,\n",
    "                           custom_patches[layer_idx][batch_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct comparison with paper implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With MLP projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_patch_sampler = PatchSampler(\n",
    "    patch_embedding_dim=patch_embedding_dim,\n",
    "    num_patches_per_layer=num_patches_per_layer,\n",
    "    device=torch.device('cpu')\n",
    ")\n",
    "paper_patch_sampler = PatchSampleF(\n",
    "    nc=patch_embedding_dim,\n",
    "    device=torch.device('cpu'),\n",
    "    use_mlp=True\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "  layer_outs = [\n",
    "      torch.randn((batch_size, 128, 128, 128)),\n",
    "      torch.randn((batch_size, 256, 64, 64)),\n",
    "      torch.randn((batch_size, 512, 32, 32)),\n",
    "  ]\n",
    "  cust_patches, cust_idx = custom_patch_sampler(layer_outs, apply_mlp=True)\n",
    "  for layer_idx in range(len(cust_patches)):\n",
    "    setattr(\n",
    "        paper_patch_sampler,\n",
    "        f'mlp_{layer_idx}',\n",
    "        getattr(\n",
    "            custom_patch_sampler,\n",
    "            f'mlp_{layer_idx}',\n",
    "        )\n",
    "    )\n",
    "  paper_patch_sampler.mlp_init = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Work/projects/lagan/paper_patch_sampler.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  patch_id = torch.tensor(patch_id, dtype=torch.long, device=feat.device)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "iters = 100\n",
    "\n",
    "for _ in range(iters):\n",
    "  layer_outs = [\n",
    "      torch.randn((batch_size, 128, 128, 128)),\n",
    "      torch.randn((batch_size, 256, 64, 64)),\n",
    "      torch.randn((batch_size, 512, 32, 32)),\n",
    "  ]\n",
    "  with torch.no_grad():\n",
    "    custom_patches, custom_idx = custom_patch_sampler(layer_outs)\n",
    "    custom_patches_2, custom_idx_2 = custom_patch_sampler(\n",
    "        layer_outs, custom_idx)\n",
    "    paper_patches, paper_idx = paper_patch_sampler(\n",
    "        feats=layer_outs,\n",
    "        num_patches=num_patches_per_layer,\n",
    "        patch_ids=[\n",
    "            idx.flatten()\n",
    "            for idx in custom_idx\n",
    "        ]\n",
    "    )\n",
    "    paper_idx = [\n",
    "        idx.reshape(batch_size, -1)\n",
    "        for idx in paper_idx\n",
    "    ]\n",
    "    for i, j in zip(custom_idx, custom_idx_2):\n",
    "      assert torch.equal(i, j)\n",
    "    for i, j in zip(custom_idx_2, paper_idx):\n",
    "      assert torch.equal(i, j)\n",
    "\n",
    "    for p, q in zip(custom_patches, custom_patches_2):\n",
    "      assert torch.equal(p, q)\n",
    "    for p, q in zip(custom_patches_2, paper_patches):\n",
    "      assert torch.allclose(p, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without MLP projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_patch_sampler = PatchSampleF(\n",
    "    nc=patch_embedding_dim,\n",
    "    device=torch.device('cpu'),\n",
    "    use_mlp=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100\n",
    "batch_size = 1\n",
    "\n",
    "for _ in range(iters):\n",
    "  layer_outs = [\n",
    "      torch.randn((batch_size, 128, 128, 128)),\n",
    "      torch.randn((batch_size, 256, 64, 64)),\n",
    "      torch.randn((batch_size, 512, 32, 32)),\n",
    "  ]\n",
    "  with torch.no_grad():\n",
    "    custom_patches, custom_idx = custom_patch_sampler(\n",
    "        layer_outs, apply_mlp=False)\n",
    "    custom_patches_2, custom_idx_2 = custom_patch_sampler(\n",
    "        layer_outs, custom_idx, apply_mlp=False)\n",
    "\n",
    "    paper_patches, paper_idx = paper_patch_sampler(\n",
    "        feats=layer_outs,\n",
    "        num_patches=num_patches_per_layer,\n",
    "        patch_ids=[\n",
    "            idx.flatten()\n",
    "            for idx in custom_idx\n",
    "        ]\n",
    "    )\n",
    "    paper_idx = [\n",
    "        idx.reshape(batch_size, -1)\n",
    "        for idx in paper_idx\n",
    "    ]\n",
    "    for i, j in zip(custom_idx, custom_idx_2):\n",
    "      assert torch.equal(i, j)\n",
    "    for i, j in zip(custom_idx_2, paper_idx):\n",
    "      assert torch.equal(i, j)\n",
    "\n",
    "    for p, q in zip(custom_patches, custom_patches_2):\n",
    "      assert torch.equal(p, q)\n",
    "    for p, q in zip(custom_patches_2, paper_patches):\n",
    "      assert torch.allclose(p, q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UGATIT-pytorch-J5qPllXS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
